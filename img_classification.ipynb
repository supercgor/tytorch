{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydoc import visiblename\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets.cifar as cifar\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from skimage import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, \"rb\") as fo:\n",
    "        dic = pickle.load(fo, encoding=\"bytes\")\n",
    "    return dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = unpickle(\"./data/CIFAR-10/cifar-10-batches-py/data_batch_1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([b'batch_label', b'labels', b'data', b'filenames'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_start = 0\n",
    "img_root = \"./data/CIFAR-10/processed/\"\n",
    "label = np.arange(0, 10)\n",
    "for la in label:\n",
    "    if not os.path.exists(f\"./data/CIFAR-10/processed/train/{la}/\"):\n",
    "        os.makedirs(f\"./data/CIFAR-10/processed/train/{la}/\")\n",
    "    if not os.path.exists(f\"./data/CIFAR-10/processed/test/{la}/\"):\n",
    "        os.makedirs(f\"./data/CIFAR-10/processed/test/{la}/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_231/2283250547.py:10: UserWarning: ./data/CIFAR-10/processed/train/2/flying_bird_s_001061.jpg is a low contrast image\n",
      "  io.imsave(img_path,data.reshape([32,32,3],order=\"F\").swapaxes(0,1))\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    order = i * 10000\n",
    "    dic = unpickle(f\"./data/CIFAR-10/cifar-10-batches-py/data_batch_{i+1}\")\n",
    "    img_contain = dic[b'data']\n",
    "    img_name = dic[b'filenames']\n",
    "    img_label = dic[b'labels']\n",
    "    for j, (label, name, data) in enumerate(zip(img_label, img_name, img_contain)):\n",
    "        name = str(name).replace(\"b'\", '').replace(\".png'\", '')\n",
    "        img_path = f\"./data/CIFAR-10/processed/train/{label}/{name}.jpg\"\n",
    "        io.imsave(img_path, data.reshape(\n",
    "            [32, 32, 3], order=\"F\").swapaxes(0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = unpickle(f\"./data/CIFAR-10/cifar-10-batches-py/test_batch\")\n",
    "img_contain = dic[b'data']\n",
    "img_name = dic[b'filenames']\n",
    "img_label = dic[b'labels']\n",
    "for j, (label, name, data) in enumerate(zip(img_label, img_name, img_contain)):\n",
    "    name = str(name).replace(\"b'\", '').replace(\".png'\", '')\n",
    "    img_path = f\"./data/CIFAR-10/processed/test/{label}/{name}.jpg\"\n",
    "    io.imsave(img_path, data.reshape([32, 32, 3], order=\"F\").swapaxes(0, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from asyncio.unix_events import BaseChildWatcher\n",
    "from random import shuffle\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "train_root = r\"./data/CIFAR-10/processed/train/\"\n",
    "test_root = r\"./data/CIFAR-10/processed/test/\"\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "test_transform = train_transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leNet\n",
    "from pyclbr import Function\n",
    "import torch.nn as nn\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "class leNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(leNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.linear1 = nn.Linear(16*5*5, 120)\n",
    "        self.linear2 = nn.Linear(120, 84)\n",
    "        self.linear3 = nn.Linear(84, 10)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = x.view(x.size()[0], -1)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.linear3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class alexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(alexNet, self).__init__()\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256*2*2, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature(x)\n",
    "        x = x.view(x.size(0), 256 * 2 * 2)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.first = nn.Sequential(\n",
    "            nn.Conv2d(3, 18, kernel_size=3, padding=1, stride=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "        )\n",
    "        self.second = nn.Sequential(\n",
    "            nn.Linear(18*16*16, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first(x)\n",
    "        x = x.view(-1, 18*16*16)\n",
    "        x = self.second(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VGGNet(nn.Module):\n",
    "    def __init__(self, mode, batch_norm=False):\n",
    "        super(VGGNet, self).__init__()\n",
    "        self.features = self.make_layers(mode, batch_norm)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "        # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, sqrt(2. / n))\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def make_layers(self, mode, batch_norm):\n",
    "        cfg = {\n",
    "            'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "            'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "            'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "            'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M',\n",
    "                  512, 512, 512, 512, 'M'],\n",
    "        }\n",
    "        cfg = cfg[mode]\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for v in cfg:\n",
    "            if v == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "                if batch_norm:\n",
    "                    layers += [conv2d,\n",
    "                               nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "                else:\n",
    "                    layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "                in_channels = v\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class NiNNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NiNNet, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            # MLP卷积层1\n",
    "            nn.Conv2d(3, 192, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(192, 160, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(160, 96, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            # MLP卷积层2\n",
    "            nn.Conv2d(96, 192, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            # MLP卷积层3\n",
    "            nn.Conv2d(192, 192, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(192, 10, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool2d(kernel_size=8, stride=1, padding=0),\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.classifier(x)\n",
    "        x = nn.functional.avg_pool2d(x, kernel_size=x.size()[2:])\n",
    "        x = x.view(x.size(0), 10)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "\n",
    "def createlossandoptimizer(net, learning_rate=0.001):\n",
    "    loss = torch.nn.CrossEntropyLoss()  # 交叉熵损失函数\n",
    "    # Adam 优化算法是随机梯度下降算法的扩展式\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    print(optimizer)\n",
    "    return loss, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import time\n",
    "\n",
    "\n",
    "def trainNet(net, batchsize, n_epochs, learning_rate=0.001):\n",
    "    net.train()\n",
    "    print(\"HYPERPARAMETERS: \")\n",
    "    print(\"batch-size= \", batchsize)\n",
    "    print(\"n_epochs= \", n_epochs)\n",
    "    print(\"learning_rate= \", learning_rate)\n",
    "\n",
    "    train_dataset = torchvision.datasets.ImageFolder(\n",
    "        root=train_root, transform=train_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batchsize,\n",
    "                              shuffle=True, num_workers=2)\n",
    "\n",
    "    n_batches = len(train_loader)  # dataset size / batch size\n",
    "    print(\"n_batches= \", n_batches)\n",
    "\n",
    "    loss, optimizer = createlossandoptimizer(net, learning_rate)\n",
    "\n",
    "    test_dataset = torchvision.datasets.ImageFolder(\n",
    "        root=test_root, transform=test_transform)\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=4,\n",
    "                             shuffle=True, num_workers=2)\n",
    "\n",
    "    training_start_time = time.time()\n",
    "\n",
    "    print(\"training start:\")\n",
    "    for epoch in range(n_epochs):\n",
    "        running_loss = 0.0\n",
    "        print_every = n_batches\n",
    "        start_time = time.time()\n",
    "        total_train_loss = 0\n",
    "        train_acc = 0\n",
    "        for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "# 将所有的梯度置零，原因是防止每次backward的时候梯度会累加\n",
    "            optimizer.zero_grad()\n",
    "            # forward\n",
    "            outputs = net(inputs)\n",
    "            _, pred = outputs.max(1)\n",
    "            num_correct = (pred == labels).sum().item()\n",
    "            acc = num_correct/batchsize\n",
    "            train_acc += acc\n",
    "            # loss\n",
    "            loss_size = loss(outputs, labels)\n",
    "            # backward\n",
    "            loss_size.backward()\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "            # print(loss_size)\n",
    "            running_loss += loss_size.item()\n",
    "            # 在一个epoch里。每十组batchsize大小的数据输出一次结果，即以batch_size大小的数据为一组，到第10组，20组，30组...的时候输出\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(\n",
    "                    f\"[Info] ====== {(i + 1)//batchsize*20}% ---- Loss {loss_size.item()/batchsize:.4f} ---- Accuracy {acc:.2f} ======\")\n",
    "        print(\n",
    "            f\"[Info] ====== Training ---- Epoch {epoch+1} ---- Average loss {running_loss/n_batches:.4f} ---- Accuracy {train_acc/len(train_loader):.4f} ======\")\n",
    "        total_val_loss = 0\n",
    "\n",
    "# 所有的Epoch结束，也就是训练结束，计算花费的时间\n",
    "    print(\"Training finished, took {:.2f}s\".format(\n",
    "        time.time() - training_start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYPERPARAMETERS: \n",
      "batch-size=  100\n",
      "n_epochs=  10\n",
      "learning_rate=  0.001\n",
      "n_batches=  500\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "training start:\n",
      "[Info] ====== 20% ---- Loss 0.0228 ---- Accuracy 0.14 ======\n",
      "[Info] ====== 40% ---- Loss 0.0227 ---- Accuracy 0.21 ======\n",
      "[Info] ====== 60% ---- Loss 0.0217 ---- Accuracy 0.23 ======\n",
      "[Info] ====== 80% ---- Loss 0.0205 ---- Accuracy 0.30 ======\n",
      "[Info] ====== 100% ---- Loss 0.0208 ---- Accuracy 0.32 ======\n",
      "[Info] ====== Training ---- Epoch 1 ---- Average loss 2.1786 ---- Accuracy 0.2348 ======\n",
      "[Info] ====== 20% ---- Loss 0.0209 ---- Accuracy 0.33 ======\n",
      "[Info] ====== 40% ---- Loss 0.0201 ---- Accuracy 0.36 ======\n",
      "[Info] ====== 60% ---- Loss 0.0206 ---- Accuracy 0.31 ======\n",
      "[Info] ====== 80% ---- Loss 0.0189 ---- Accuracy 0.35 ======\n",
      "[Info] ====== 100% ---- Loss 0.0188 ---- Accuracy 0.38 ======\n",
      "[Info] ====== Training ---- Epoch 2 ---- Average loss 1.9329 ---- Accuracy 0.3759 ======\n",
      "[Info] ====== 20% ---- Loss 0.0170 ---- Accuracy 0.47 ======\n",
      "[Info] ====== 40% ---- Loss 0.0162 ---- Accuracy 0.51 ======\n",
      "[Info] ====== 60% ---- Loss 0.0178 ---- Accuracy 0.47 ======\n",
      "[Info] ====== 80% ---- Loss 0.0184 ---- Accuracy 0.44 ======\n",
      "[Info] ====== 100% ---- Loss 0.0175 ---- Accuracy 0.46 ======\n",
      "[Info] ====== Training ---- Epoch 3 ---- Average loss 1.7988 ---- Accuracy 0.4219 ======\n",
      "[Info] ====== 20% ---- Loss 0.0203 ---- Accuracy 0.28 ======\n",
      "[Info] ====== 40% ---- Loss 0.0178 ---- Accuracy 0.39 ======\n",
      "[Info] ====== 60% ---- Loss 0.0166 ---- Accuracy 0.44 ======\n",
      "[Info] ====== 80% ---- Loss 0.0177 ---- Accuracy 0.39 ======\n",
      "[Info] ====== 100% ---- Loss 0.0160 ---- Accuracy 0.51 ======\n",
      "[Info] ====== Training ---- Epoch 4 ---- Average loss 1.7172 ---- Accuracy 0.4457 ======\n",
      "[Info] ====== 20% ---- Loss 0.0175 ---- Accuracy 0.44 ======\n",
      "[Info] ====== 40% ---- Loss 0.0176 ---- Accuracy 0.41 ======\n",
      "[Info] ====== 60% ---- Loss 0.0161 ---- Accuracy 0.46 ======\n",
      "[Info] ====== 80% ---- Loss 0.0148 ---- Accuracy 0.49 ======\n",
      "[Info] ====== 100% ---- Loss 0.0156 ---- Accuracy 0.49 ======\n",
      "[Info] ====== Training ---- Epoch 5 ---- Average loss 1.6513 ---- Accuracy 0.4613 ======\n",
      "[Info] ====== 20% ---- Loss 0.0148 ---- Accuracy 0.54 ======\n",
      "[Info] ====== 40% ---- Loss 0.0161 ---- Accuracy 0.46 ======\n",
      "[Info] ====== 60% ---- Loss 0.0147 ---- Accuracy 0.54 ======\n",
      "[Info] ====== 80% ---- Loss 0.0147 ---- Accuracy 0.52 ======\n",
      "[Info] ====== 100% ---- Loss 0.0154 ---- Accuracy 0.56 ======\n",
      "[Info] ====== Training ---- Epoch 6 ---- Average loss 1.5990 ---- Accuracy 0.4778 ======\n",
      "[Info] ====== 20% ---- Loss 0.0133 ---- Accuracy 0.61 ======\n",
      "[Info] ====== 40% ---- Loss 0.0137 ---- Accuracy 0.54 ======\n",
      "[Info] ====== 60% ---- Loss 0.0154 ---- Accuracy 0.46 ======\n",
      "[Info] ====== 80% ---- Loss 0.0133 ---- Accuracy 0.57 ======\n",
      "[Info] ====== 100% ---- Loss 0.0149 ---- Accuracy 0.49 ======\n",
      "[Info] ====== Training ---- Epoch 7 ---- Average loss 1.5620 ---- Accuracy 0.4853 ======\n",
      "[Info] ====== 20% ---- Loss 0.0157 ---- Accuracy 0.46 ======\n",
      "[Info] ====== 40% ---- Loss 0.0149 ---- Accuracy 0.48 ======\n",
      "[Info] ====== 60% ---- Loss 0.0148 ---- Accuracy 0.50 ======\n",
      "[Info] ====== 80% ---- Loss 0.0161 ---- Accuracy 0.51 ======\n",
      "[Info] ====== 100% ---- Loss 0.0144 ---- Accuracy 0.48 ======\n",
      "[Info] ====== Training ---- Epoch 8 ---- Average loss 1.5228 ---- Accuracy 0.4967 ======\n",
      "[Info] ====== 20% ---- Loss 0.0156 ---- Accuracy 0.49 ======\n",
      "[Info] ====== 40% ---- Loss 0.0156 ---- Accuracy 0.50 ======\n",
      "[Info] ====== 60% ---- Loss 0.0170 ---- Accuracy 0.42 ======\n",
      "[Info] ====== 80% ---- Loss 0.0152 ---- Accuracy 0.50 ======\n",
      "[Info] ====== 100% ---- Loss 0.0146 ---- Accuracy 0.54 ======\n",
      "[Info] ====== Training ---- Epoch 9 ---- Average loss 1.4970 ---- Accuracy 0.5048 ======\n",
      "[Info] ====== 20% ---- Loss 0.0145 ---- Accuracy 0.51 ======\n",
      "[Info] ====== 40% ---- Loss 0.0131 ---- Accuracy 0.53 ======\n",
      "[Info] ====== 60% ---- Loss 0.0123 ---- Accuracy 0.60 ======\n",
      "[Info] ====== 80% ---- Loss 0.0117 ---- Accuracy 0.61 ======\n",
      "[Info] ====== 100% ---- Loss 0.0153 ---- Accuracy 0.47 ======\n",
      "[Info] ====== Training ---- Epoch 10 ---- Average loss 1.4650 ---- Accuracy 0.5132 ======\n",
      "Training finished, took 454.10s\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0')\n",
    "\n",
    "net = NiNNet().to(device)\n",
    "\n",
    "trainNet(net, 100, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('tytorch': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b54fb8822f8bd57b64387997ebc5890a6df68cdeb31085e8585fa83fd840068"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
